---
title: How I Turned an AI Skeptic and a 13-Year-Old Into Believers
description: >-
  Two conversion stories that prove AI literacy isn't about technical
  skills—it's about showing people the right use case.
pubDate: 2026-02-21T00:00:00.000Z
tags:
  - AI
  - GitHub Copilot
  - Developer Experience
  - Career
  - Productivity
draft: false
devto_id: 3272191
devto_hash: 52bb9d1505052a6c
hashnode_id: 69990fb621b653b5dbcf921b
hashnode_hash: 2f45265cfc5eccb0
---

I met a guy at the park last week who only uses AI for one thing: naming his cook-off. That's it. Full stop.

When I asked why, he said he just doesn't see the point. He's not anti-tech—he's just genuinely unconvinced that AI has anything to offer him beyond generating clever names for food events. And here's the uncomfortable truth: there's a whole group of individuals that is completely blind to what's happening in the AI space. Not because they're stupid or resistant to change, but because nobody has shown them *their* use case.

That conversation stuck with me because I'd just spent the previous weekend watching two people discover AI in completely different ways. One was my father-in-law, who needs help operating his phone. The other was my 13-year-old brother-in-law, who'd never written a line of code. By the end of the weekend, one had discovered famous people in his family history through an AI conversation, and the other had shipped working games to GitHub.

These aren't fairy tales about AI magic. They're stories about meeting people where they are and showing them exactly one thing that changes everything.

## The Interview Technique: When AI Learns About You

My father-in-law is the demographic that tech evangelists love to write off. Older generation. Needs help with basic phone operations. Probably won't "get" new technology. You know the stereotype.

His first questions to AI were exactly what you'd expect: Google-style factual queries about things he already knew the answers to. "What year did the Apollo 11 mission happen?" That kind of thing. He was testing it, treating it like a search engine, getting back information that confirmed what he'd memorized decades ago.

I suggested something different: "Why don't you ask it to interview *you* about your life and family history?"

That shifted everything. Instead of AI being a reference tool for facts he already knew, it became a conversation partner asking him about his experiences, his family, where they came from. The AI was learning about him, building context, asking follow-up questions the way a genealogist or journalist would.

Then he asked one of his original questions again.

This time, the AI didn't just give him the factual answer. It tied the response directly to his personal story. It connected historical events to his family timeline. It surfaced relationships he hadn't considered. And buried in that conversation, it helped him discover famous people in his family tree that he'd never known about.

He's never going to Google anything again.

This is what [personalized AI interactions](https://arxiv.org/html/2512.12686v1) look like when they work. Research from MIT shows that context in long-term conversations causes LLMs to [mirror user viewpoints and preferences](https://news.mit.edu/2026/personalization-features-can-make-llms-more-agreeable-0218), creating responses that feel genuinely relevant. My father-in-law didn't care about the technology—he cared that something finally understood him well enough to connect dots he couldn't see.

The barrier wasn't technical capability. It was use case clarity. He needed to see one thing AI could do *for him* that nothing else could.

## A New Developer Was Just Made

My 13-year-old brother-in-law is a gamer. Never programmed. Never expressed interest in programming. Just plays games and understands game mechanics the way gamers do—intuitively, from thousands of hours of pattern recognition.

I gave him [GitHub Copilot CLI](https://githubnext.com/projects/copilot-cli) and one challenge: build something.

Within an hour, he'd created a 3D Minecraft-style game. Not a tutorial. Not copying code. He was *iterating*. He added enemies. Then shooting mechanics. Then transformed the basic game into something more complex.

Being a gamer turned out to be the perfect foundation. He already knew all the keystrokes, all the interaction patterns. What he didn't know was syntax, architecture, or "best practices." And honestly? That didn't matter.

He created actual GitHub repos: a Battle Royale game, a 3D Geometry Dash clone. Real projects with real commit history. He wasn't learning programming the way I learned it in college—memorizing syntax, debugging cryptic error messages, slowly building mental models of how computers work.

He was learning by articulating what he wanted and watching AI translate it into reality. Then tweaking it. Then articulating the next thing.

Research on [AI-assisted coding for young learners](https://www.raspberrypi.org/blog/using-an-ai-code-generator-with-school-age-beginner-programmers/) shows this isn't an outlier. Studies with school-age programmers demonstrate that AI copilots help students "focus on the problem-solving aspects of computing" rather than getting stuck on syntax. A [workshop with 14-year-olds using GitHub Copilot CLI](https://techcommunity.microsoft.com/blog/educatordeveloperblog/from-zero-to-16-games-in-2-hours/4492813) produced 16 fully functional games in two hours using one-shot prompting techniques.

Here's my controversial take: AI is teaching a different way of articulating thoughts that might be more valuable than traditional schooling for certain types of learners. Not for everyone. Not for every domain. But for a 13-year-old who thinks in game mechanics and wants to build, this path is legitimately viable.

A new developer was just made. Not through CS101. Through iteration, feedback, and clear articulation of intent.

## The AI Literacy Gap Nobody Talks About

Here's what frustrates me: [90% of workers report using AI tools](https://www.udacity.com/blog/2025/09/ai-adoption-trust-gap-udacity-research.html), but trust remains low and abandonment rates are high. That gap isn't about AI capability—it's about AI literacy. And AI literacy isn't just knowing *how* to use tools. It's knowing *when* and *why* they're useful.

The guy at the park, my father-in-law, my brother-in-law—they all started from the same place: unclear on why AI mattered to them personally. One is still stuck there. Two broke through.

The difference wasn't intelligence or tech-savviness. It was finding the one use case that clicked.

Research shows a [generational crossroads in AI adoption](https://rodtrent.substack.com/p/generative-ai-adoption-a-generational), with older generations resisting due to unfamiliarity while younger generations show mixed feelings despite higher adoption rates. But that framing misses the point. Age isn't the barrier—relevance is.

My father-in-law is in the demographic that supposedly resists AI. But he doesn't resist AI anymore because someone showed him a use case that connected to something he deeply cared about: his family history.

My brother-in-law is in the demographic that supposedly embraces AI. But he wouldn't have if the tool didn't map to skills he already had and goals he already wanted.

## The Challenge

Find one AI skeptic in your life and show them AI magic. Not a demo. Not a features tour. Find *their* use case and show them exactly one thing that changes how they think about what's possible.

Find one young person and give them creative tools. Not a curriculum. Not a structured lesson plan. Give them [agency and scaffolding](https://arxiv.org/abs/2505.03867) to build something they actually want to make.

You'll know it worked when they start iterating on their own. When they stop asking you for help and start asking AI for the next step. When they realize the constraint isn't capability anymore—it's imagination.

I've written before about [building the future with AI](/articles/building-the-future-with-ai) and how [developer fulfillment](/articles/copilot-developer-fulfillment) changes when tools amplify intent rather than just autocomplete code. But the real unlock isn't technical—it's pedagogical.

We're not just teaching people to use AI. We're teaching them to articulate intent clearly enough that intelligence—artificial or otherwise—can help them achieve it. And that skill, that way of thinking, might be the most valuable thing anyone can learn right now.

## The Bottom Line

The AI literacy gap isn't about access to tools. It's about access to relevant use cases and people willing to meet learners where they are.

My father-in-law didn't need a tutorial on prompt engineering. He needed someone to suggest asking AI to interview him.

My brother-in-law didn't need CS fundamentals. He needed permission to build and a tool that spoke his language.

The skeptic at the park still only uses AI to name his cook-off. But I'm willing to bet if someone found his use case—if someone showed him the one thing AI could do that genuinely changed his workflow or solved his problem—he'd stop being a skeptic.

We're living through a transition period where the technology is ready but the humans aren't yet. Not because they can't learn, but because we're still figuring out how to teach. The playbook for [context engineering](/articles/context-engineering-key-to-ai-development) and effective AI interaction is being written in real-time by people who are willing to experiment.

Your move: find your skeptic. Find your young builder. Show them their use case. Watch what happens.
